checkpoint.interval = 180000
job.parallelism = 1

generator = {
  startTime_min = 1580835600000
  startTime_max = 1707066000000
  rowsPerSecond = 1500
  numberOfRows = -1
  imsiNotNullProbability = 0.5
  msisdnNotNullProbability = 0.1
}


kafka = {
  format = "csv"
  topic = "test3"
  bootstrap.servers = "kafka:9092"
  group_id = "spark-group"
  failOnDataLoss = "false"
  scan.startup.mode = "earliest-offset"
}

ms_ip = {
  format = "jdbc"
  url = "jdbc:postgresql://host.docker.internal:5432/diploma"
  dbtable = "public.ms_ip"
  user = "postgres"
  password = "7844"
}

imsi_msisdn = {
  format = "jdbc"
  url = "jdbc:postgresql://host.docker.internal:5432/diploma"
  dbtable = "public.imsi_msisdn"
  user = "postgres"
  password = "7844"
  lookup.partial-cache.max-rows = "100"
  lookup.partial-cache.expire-after-write = "60s"
}

ms_ip_exploded = {
  format = "jdbc"
  url = "jdbc:postgresql://host.docker.internal:5432/diploma"
  dbtable = "public.ms_ip_exploded"
  user = "postgres"
  password = "7844"
  lookup.partial-cache.max-rows = "100"
  lookup.partial-cache.expire-after-write = "60s"
}

hdfs = {
    name = "hdfs_sink"
    partitionBy = ["event_date", "probe"]
    blockSize = "128MB"
    fileSize = "110MB"
    format = "parquet"
    path = "hdfs://namenode:8020/spark/results"
    checkpointLocation = "hdfs://namenode:8020/flink/checkpoints"
}
