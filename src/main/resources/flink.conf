checkpointing.interval = 1000
job.parallelism = 1

generator = {
  startTime_min = 1580835600000
  startTime_max = 1707066000000
  rowsPerSecond = 5
  numberOfRows = 1200
  imsiNotNullProbability = 0.5
  msisdnNotNullProbability = 0.1
}


kafka = {
  format = "csv"
  topic = "test3"
  bootstrap.servers = "kafka:9092"
  group_id = "spark-group"
  failOnDataLoss = "false"
  scan.startup.mode = "earliest-offset"
}

ms_ip = {
  format = "jdbc"
  url = "jdbc:postgresql://host.docker.internal:5432/diploma"
  dbtable = "public.ms_ip"
  user = "postgres"
  password = "7844"
}

imsi_msisdn = {
  format = "jdbc"
  url = "jdbc:postgresql://host.docker.internal:5432/diploma"
  dbtable = "public.imsi_msisdn"
  user = "postgres"
  password = "7844"
}

hdfs = {
    name = "hdfs_sink"
    partitionBy = ["event_date", "probe"]
    blockSize = "128MB"
    fileSize = "110MB"
    format = "parquet"
    path = "hdfs://namenode:8020/spark/results"
    checkpointLocation = "hdfs://namenode:8020/spark/checkpoints"
}
